{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c370bd-3d2c-4554-8471-a2b9109b371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 1\n",
    " #ans -- The mathematical formula for a linear Support Vector Machine (SVM) can be described as follows:\n",
    "\n",
    "Given a dataset with N data points represented as (x_i, y_i), where x_i is the feature vector for the i-th data point, and y_i is the corresponding class label (-1 for the negative class, 1 for the positive class), the goal of a linear SVM is to find a hyperplane that best separates the two classes while maximizing the margin between them.\n",
    "\n",
    "The hyperplane can be represented by the equation:\n",
    "\n",
    "w · x + b = 0\n",
    "\n",
    "Where:\n",
    "- \"w\" is the weight vector perpendicular to the hyperplane.\n",
    "- \"x\" is the feature vector of a data point.\n",
    "- \"b\" is the bias term, which shifts the hyperplane away from the origin.\n",
    "\n",
    "The decision function of the SVM can be defined as:\n",
    "\n",
    "f(x) = sign(w · x + b)\n",
    "\n",
    "Here, \"f(x)\" gives you the predicted class label for a given input feature vector \"x.\" If f(x) is positive, it assigns the positive class label, and if it's negative, it assigns the negative class label.\n",
    "\n",
    "The SVM aims to find the \"w\" and \"b\" that maximize the margin between the two classes while ensuring that all data points are correctly classified. This is typically formulated as an optimization problem, often referred to as the \"soft-margin\" SVM formulation when dealing with non-linearly separable data.\n",
    "\n",
    "The optimization problem can be expressed as:\n",
    "\n",
    "Minimize: (1/2) * ||w||^2 + C * Σ(max(0, 1 - y_i * (w · x_i + b)))\n",
    "\n",
    "Subject to: ∀i, y_i * (w · x_i + b) ≥ 1\n",
    "\n",
    "Where:\n",
    "- \"C\" is the regularization parameter that controls the trade-off between maximizing the margin and minimizing classification errors.\n",
    "- The summation Σ runs over all data points in the dataset.\n",
    "\n",
    "The objective is to find \"w\" and \"b\" that minimize this objective function while satisfying the constraint that all data points are correctly classified (or very few are misclassified if the data is not perfectly separable). This optimization problem is typically solved using techniques like quadratic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4b2c9-6ec1-455f-9949-d6452611d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 2 \n",
    "# Ans-- The objective function of a linear Support Vector Machine (SVM) is used to define the optimization problem that the SVM aims to solve. The primary goal of this objective function is to find the parameters (weight vector \"w\" and bias term \"b\") of the linear hyperplane that best separates the two classes while maximizing the margin between them. Additionally, it accounts for the correct classification of training data points.\n",
    "\n",
    "The objective function for a linear SVM can be expressed as follows:\n",
    "\n",
    "Minimize: (1/2) * ||w||^2 + C * Σ(max(0, 1 - y_i * (w · x_i + b)))\n",
    "\n",
    "Subject to: ∀i, y_i * (w · x_i + b) ≥ 1\n",
    "\n",
    "Let's break down the components of this objective function:\n",
    "\n",
    "1. (1/2) * ||w||^2: This term represents the regularization part of the objective. It seeks to minimize the squared Euclidean norm (L2 norm) of the weight vector \"w.\" The regularization term (1/2) * ||w||^2 encourages finding a solution with a smaller weight vector, which helps maximize the margin between the classes and avoid overfitting.\n",
    "\n",
    "2. C * Σ(max(0, 1 - y_i * (w · x_i + b))): This term represents the classification part of the objective. It sums over all the training data points (Σ) and introduces a hinge loss function: max(0, 1 - y_i * (w · x_i + b)). The hinge loss is zero for correctly classified data points (when y_i * (w · x_i + b) ≥ 1) and increases linearly as data points get misclassified. The regularization parameter \"C\" controls the trade-off between maximizing the margin and minimizing classification errors. A larger \"C\" value places more emphasis on correct classification, potentially allowing some data points to fall inside the margin or even on the wrong side of the hyperplane, while a smaller \"C\" value focuses more on maximizing the margin.\n",
    "\n",
    "3. Subject to: ∀i, y_i * (w · x_i + b) ≥ 1: This is a constraint that ensures that all data points are correctly classified or, more precisely, that they lie on the correct side of the decision boundary with a margin of at least 1. This constraint is crucial for defining the margin and ensuring the SVM's robustness.\n",
    "\n",
    "In summary, the objective function of a linear SVM balances two goals: maximizing the margin (controlled by the regularization term) and minimizing classification errors (controlled by the hinge loss term with the regularization parameter \"C\"). The optimization problem aims to find the \"w\" and \"b\" that minimize this objective function while satisfying the constraint that all data points are correctly classified or within the margin. This results in a hyperplane that effectively separates the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f54d3-53fe-4ccd-ae87-00d2fc244667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 3\n",
    "# ans --  The kernel trick is a fundamental concept in Support Vector Machines (SVMs), a popular machine learning algorithm used for classification and regression tasks. It allows SVMs to handle non-linearly separable data by implicitly mapping the input data into a higher-dimensional feature space where the data may become linearly separable. This enables SVMs to effectively learn complex decision boundaries that wouldn't be possible in the original input space.\n",
    "\n",
    "Here's how the kernel trick works:\n",
    "\n",
    "1. **Linear SVM**: In its simplest form, an SVM tries to find a hyperplane in the input feature space that best separates the data into different classes. The hyperplane is chosen to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class. Mathematically, this can be represented as:\n",
    "\n",
    "    \\[w^Tx + b = 0\\]\n",
    "\n",
    "    Where:\n",
    "    - \\(w\\) is the weight vector.\n",
    "    - \\(x\\) is the input data vector.\n",
    "    - \\(b\\) is the bias term.\n",
    "\n",
    "2. **Non-Linear Data**: However, in real-world scenarios, data is often not linearly separable. To handle such data, the kernel trick comes into play. Instead of explicitly mapping the data to a higher-dimensional space (which can be computationally expensive or even impractical for very high dimensions), the kernel trick introduces a function called a \"kernel.\"\n",
    "\n",
    "3. **Kernel Functions**: A kernel function (\\(K\\)) computes the dot product between the feature vectors in the higher-dimensional space without explicitly calculating the transformation. The most commonly used kernel functions include:\n",
    "   - Linear Kernel: \\(K(x, x') = x^Tx'\\)\n",
    "   - Polynomial Kernel: \\(K(x, x') = (x^Tx' + c)^d\\), where \\(c\\) is a constant and \\(d\\) is the degree.\n",
    "   - Radial Basis Function (RBF) Kernel (Gaussian Kernel): \\(K(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right)\\), where \\(\\sigma\\) is a bandwidth parameter.\n",
    "\n",
    "4. **Kernel Trick**: Instead of explicitly mapping \\(x\\) and \\(x'\\) to a higher-dimensional space and then computing the dot product, you can directly compute \\(K(x, x')\\) in the original input space. The SVM optimization problem remains the same, but the kernel trick allows you to implicitly work in the higher-dimensional space.\n",
    "\n",
    "   So, the decision boundary becomes:\n",
    "\n",
    "   \\[w^T\\phi(x) + b = 0\\]\n",
    "\n",
    "   Where \\(\\phi(x)\\) is the implicit mapping of \\(x\\) to the higher-dimensional space, and \\(\\phi(x')\\) is the implicit mapping of \\(x'\\).\n",
    "\n",
    "Using the kernel trick, SVMs can effectively model non-linear relationships in the data, making them a powerful tool for various machine learning tasks. The choice of the appropriate kernel function is crucial and depends on the specific characteristics of the data you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee22dec-9ef4-46c1-a944-7967f45e85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 4 \n",
    "# ans --  In Support Vector Machines (SVM), support vectors play a critical role in defining the decision boundary and maximizing the margin between different classes. They are the data points that are closest to the decision boundary and are used to determine the position and orientation of the separating hyperplane. Let's explain the role of support vectors with an example:\n",
    "\n",
    "**Example: Binary Classification of Two Classes**\n",
    "\n",
    "Suppose you have a binary classification problem with two classes, represented as blue circles and red squares on a two-dimensional plane:\n",
    "\n",
    "- Blue Circles (Class A): (1, 2), (2, 3), (2, 5), (3, 2), (4, 3)\n",
    "- Red Squares (Class B): (6, 5), (7, 5), (7, 7), (8, 6), (9, 7)\n",
    "\n",
    "The goal of SVM is to find a hyperplane that best separates these two classes. In this example, let's assume we use a linear kernel, so we are looking for a linear decision boundary (a line in this 2D space).\n",
    "\n",
    "Now, let's visualize these data points and the decision boundary:\n",
    "\n",
    "```\n",
    "       Class A (Blue Circles)\n",
    "             |\n",
    "             |\n",
    "             |       * (2, 5)\n",
    "             |      /\n",
    "             |     /\n",
    "             |    * (2, 3)\n",
    "             |   /\n",
    "             |  /       * (6, 5)\n",
    "             | /        /\n",
    "             |/________/\n",
    "             | * (1, 2)   * (7, 7)\n",
    "             |/  /   * (4, 3)\n",
    "             |   /\n",
    "             |  /\n",
    "             | /\n",
    "             |/____________ Class B (Red Squares)\n",
    "             |\n",
    "---------------------------------------------\n",
    "```\n",
    "\n",
    "In this case, the decision boundary is the line that separates the two classes. It might look something like this:\n",
    "\n",
    "```\n",
    "             |\n",
    "             |\n",
    "             |       * (2, 5)\n",
    "             |      /\n",
    "             |     /\n",
    "             |    * (2, 3)\n",
    "             |   /|\n",
    "             |  / |\n",
    "             | /  |\n",
    "             |/___|________\n",
    "             |       * (6, 5)\n",
    "             |      / \n",
    "             |     /\n",
    "             |    * (7, 7)\n",
    "             |   /  \n",
    "             |  /\n",
    "             | /   \n",
    "             |/____________\n",
    "             |\n",
    "---------------------------------------------\n",
    "```\n",
    "\n",
    "Now, let's identify the support vectors:\n",
    "\n",
    "1. **Support Vector 1**: The blue circle at (2, 3) is a support vector from Class A. It's the closest data point to the decision boundary from Class A.\n",
    "\n",
    "2. **Support Vector 2**: The blue circle at (2, 5) is another support vector from Class A. It's also a point closest to the decision boundary from Class A.\n",
    "\n",
    "3. **Support Vector 3**: The red square at (6, 5) is a support vector from Class B. It's the closest data point to the decision boundary from Class B.\n",
    "\n",
    "These support vectors are crucial because they determine the position and orientation of the separating hyperplane. The margin of the SVM is defined by the distance between this hyperplane and the support vectors. SVM aims to maximize this margin while still correctly classifying the data points.\n",
    "\n",
    "In summary, support vectors are the key data points that directly influence the construction of the decision boundary in SVM. They are the ones closest to the boundary and are used to define the optimal separation between different classes, ensuring a wider margin and better generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061faa2-4f0e-46f0-86d4-9a946ba105c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 5 \n",
    "# ans -- To illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM), we'll use a simple 2D example with two classes. We'll create graphs to visualize these concepts.\n",
    "\n",
    "**Example: Binary Classification with SVM**\n",
    "\n",
    "Suppose we have two classes, Class A (represented by blue circles) and Class B (represented by red squares), in a two-dimensional feature space. We want to find a decision boundary (hyperplane) that separates these two classes. Here's how each concept is illustrated:\n",
    "\n",
    "1. **Hyperplane**:\n",
    "   \n",
    "   The hyperplane is the decision boundary that separates the two classes. In a 2D space, it's a straight line. The goal is to find the optimal hyperplane that maximizes the margin between the classes. Here's a graph illustrating the hyperplane:\n",
    "\n",
    "   \n",
    "\n",
    "   In this graph, the black line is the hyperplane. It separates Class A (blue circles) from Class B (red squares).\n",
    "\n",
    "2. **Marginal Plane**:\n",
    "\n",
    "   The marginal plane, also known as the supporting hyperplane, is the plane parallel to the hyperplane that touches or just touches the nearest data points from each class. It plays a crucial role in defining the margin.\n",
    "\n",
    "   \n",
    "\n",
    "   In this graph, the dashed lines represent the marginal plane. They touch the nearest data points from Class A and Class B. The margin is the distance between these two marginal planes.\n",
    "\n",
    "3. **Hard Margin**:\n",
    "\n",
    "   A hard margin SVM aims to find a decision boundary (hyperplane) that perfectly separates the two classes without allowing any misclassification. This means that all data points are correctly classified, and there is no overlap between the classes.\n",
    "\n",
    "   \n",
    "\n",
    "   In this graph, the hyperplane (black line) separates the classes with a wide margin, and there are no data points inside the margin. This is a hard margin SVM.\n",
    "\n",
    "4. **Soft Margin**:\n",
    "\n",
    "   In some cases, it's not possible to find a hard margin due to noisy or overlapping data. A soft margin SVM allows for a certain degree of misclassification to achieve a better overall margin. It introduces the concept of \"slack variables\" to handle misclassified points.\n",
    "\n",
    "   \n",
    "\n",
    "   In this graph, the hyperplane (black line) still separates the classes but allows for a few misclassified points (circled in red). The margin is narrower than in the hard margin case, but it provides better generalization to noisy data.\n",
    "\n",
    "In practice, the choice between hard margin and soft margin depends on the nature of the data. Hard margin SVMs are suitable when the data is well-separated and noise-free, while soft margin SVMs are more robust when there is some overlap or noise in the data, as they can handle misclassified points. The parameter C in SVM controls the trade-off between maximizing the margin and minimizing the misclassification of data points in soft margin SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa885dfc-b32e-4f52-9eae-1f266b751a1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iris\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.dataset'"
     ]
    }
   ],
   "source": [
    "from sklearn.dataset import iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb201b-1935-468e-9c3e-e388d68add25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b3cc4-1027-4d77-b33e-2d94afc77be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8670286-d4e9-47a8-91e4-c76796db732d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6af673-65bf-4a86-b841-7c1b4d4a08af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6788e-c131-466b-9758-7f24d81f5293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b80a8-8531-46ec-b7c6-02f72a01d2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c1a1c-7b22-443f-8362-006bf4d5e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac863b8-5727-4aed-9f93-614a4d68f184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
